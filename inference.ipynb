{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision import datasets, models\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('/content/drive/My Drive/Colab Notebooks/P1_Facial_Keypoints/saved_models/model2.pt', map_location='cpu'))\n",
    "model.eval()\n",
    "print(\"Model succefully loaded !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rescale_img(image, output_size):\n",
    "  \n",
    "    h, w = image.shape[:2]\n",
    "\n",
    "    if isinstance(output_size, int):\n",
    "        if h > w:\n",
    "            new_h, new_w = output_size * h / w, output_size\n",
    "        else:\n",
    "            new_h, new_w = output_size, output_size * w / h\n",
    "    else:\n",
    "        new_h, new_w = output_size\n",
    "\n",
    "    new_h, new_w = int(new_h), int(new_w)\n",
    "\n",
    "    img = cv2.resize(image, (new_w, new_h))\n",
    "\n",
    "    return img\n",
    "      \n",
    "  \n",
    "  \n",
    "  \n",
    "def normalize_img(image):\n",
    "  \n",
    "    image_copy = np.copy(image)\n",
    "    image_copy = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    image_copy =  image_copy/255.0\n",
    "\n",
    "    return image_copy\n",
    "  \n",
    "  \n",
    "\n",
    "def tensor_img(image):\n",
    "\n",
    "    # if image has no grayscale color channel, add one\n",
    "    if(len(image.shape) == 2):\n",
    "        # add that third color dim\n",
    "        image = image.reshape(image.shape[0], image.shape[1], 1)\n",
    "\n",
    "    # swap color axis because\n",
    "    # numpy image: H x W x C\n",
    "    # torch image: C X H X W\n",
    "    image = image.transpose((2, 0, 1))\n",
    "\n",
    "    return torch.from_numpy(image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select region of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect Face in an Image using Cascade File\n",
    "\n",
    "image = cv2.imread('/content/drive/My Drive/Colab Notebooks/P1_Facial_Keypoints/moi2.jpg')\n",
    "\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img_detection = image.copy()\n",
    "\n",
    "\n",
    "face_cascade = cv2.CascadeClassifier('/content/drive/My Drive/Colab Notebooks/P1_Facial_Keypoints/detector_architectures/haarcascade_frontalface_default.xml')\n",
    "\n",
    "faces = face_cascade.detectMultiScale(img_detection, scaleFactor=1.2, minNeighbors=2)\n",
    "\n",
    "\n",
    "for (x,y,w,h) in faces:\n",
    "    \n",
    "    cv2.rectangle(img_detection,(x,y),(x+w,y+h),(255,0,0),3)\n",
    "\n",
    "    \n",
    "\n",
    "fig = plt.figure(figsize=(9,9))\n",
    "\n",
    "plt.axis('off')\n",
    "\n",
    "plt.imshow(img_detection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "model.cpu()\n",
    "\n",
    "# Loop over the detected faces\n",
    "# Keep the face part using cropping\n",
    "# Predict keypoint\n",
    "for (x,y,w,h) in faces:\n",
    "  \n",
    "    image_kpts = img_detection[y:y+h, x:x+w]  \n",
    "    \n",
    "\n",
    "image_kpts = cv2.resize(image_kpts, (224,224))\n",
    "\n",
    "image_kpts = normalize_img(image_kpts)\n",
    "\n",
    "image_kpts = tensor_img(image_kpts)\n",
    "\n",
    "\n",
    "# convert images to FloatTensors\n",
    "image_kpts = image_kpts.type(torch.FloatTensor)\n",
    "\n",
    "# forward pass to get model output\n",
    "with torch.no_grad():\n",
    "    output_pts = model(image_kpts.unsqueeze(0))\n",
    "\n",
    "# reshape to batch_size x 68 x 2 pts\n",
    "output_pts = output_pts.view(output_pts.size()[0], 68, -1)\n",
    "\n",
    "#unormalize images\n",
    "image_kpts = image_kpts.numpy()\n",
    "image_kpts = np.transpose(image_kpts, (1, 2, 0))\n",
    "\n",
    "\n",
    "#unormalize labels\n",
    "labels = output_pts.numpy()\n",
    "labels = labels*50.0+100\n",
    "labels = labels.reshape(-1, 2)\n",
    "\n",
    "\n",
    "#plt.subplot(4,5,i+1)\n",
    "plt.axis('off')\n",
    "plt.imshow(np.squeeze(image_kpts), cmap='gray')\n",
    "plt.scatter(labels[:, 0], labels[:, 1], s=60, marker='.', c='m')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize some filter in the first conv layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(25,20))\n",
    "\n",
    "# weight of the first conv layer\n",
    "weights1 = model.conv1.weight.data\n",
    "w = weights1.numpy()\n",
    "\n",
    "#plot the first 10 filters\n",
    "for i in range(10):\n",
    "  #choose a filter\n",
    "  num_filter = i\n",
    "\n",
    "  filter = w[num_filter][0]\n",
    "  \n",
    "  plt.subplot(1, 10, i+1)\n",
    "  plt.axis('off')\n",
    "  \n",
    "  plt.title(f\"Filter {i+1}\")\n",
    "  plt.imshow(filter, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualize some feature maps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(29,20))\n",
    "\n",
    "# Let's visualize some features maps\n",
    "\n",
    "image = cv2.imread('/content/drive/My Drive/Colab Notebooks/P1_Facial_Keypoints/moi2.jpg')\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "#plt.imshow(image)\n",
    "#plt.title(\"Orginal Image\")\n",
    "\n",
    "weights1 = model.conv1.weight.data\n",
    "w = weights1.numpy()\n",
    "\n",
    "# Applying the image\n",
    "for i in range(3):\n",
    "    num_filter = i\n",
    "\n",
    "    filtered = cv2.filter2D(image, -1, w[num_filter][0])\n",
    "\n",
    "    plt.subplot(1, 10, i+1)\n",
    "    plt.axis('off')\n",
    "\n",
    "    plt.title(f\"Feature Map filter {i+1}\")\n",
    "    plt.imshow(filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Try some funny stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keypoints position\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "\n",
    "plt.axis('off')\n",
    "plt.imshow(np.squeeze(image_kpts), cmap='gray')\n",
    "plt.scatter(labels[:, 0], labels[:, 1], s=60, marker='.', c='m', alpha = 0.5)\n",
    "\n",
    "for i in range(labels.shape[0]):\n",
    "    plt.text(labels[i,0], labels[i,1], str(i), color='white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in sunglasses image with cv2 and IMREAD_UNCHANGED because we want to read the alpha channel png\n",
    "sunglasses = cv2.imread('/content/drive/My Drive/Colab Notebooks/P1_Facial_Keypoints/images/sunglasses.png', cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis('off')\n",
    "plt.imshow(sunglasses)\n",
    "print('Image shape: ', sunglasses.shape)\n",
    "\n",
    "# print out the sunglasses transparency (alpha) channel\n",
    "alpha_channel = sunglasses[:,:,3]\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis('off')\n",
    "plt.imshow(alpha_channel, cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (x,y,w,h) in faces:\n",
    "    img = img_detection[y:y+h, x:x+w]\n",
    "\n",
    "\n",
    "# point (x,y) representant la pointe du sourcil gauche\n",
    "x = int(labels[17, 0])\n",
    "y = int(labels[17, 1])\n",
    "\n",
    "\n",
    "# h and w that we want for the sunglasse\n",
    "h = int(abs(labels[27,1] - labels[34,1]))\n",
    "w = int(abs(labels[17,0] - labels[26,0]))\n",
    "\n",
    "\n",
    "new_sunglasses =  cv2.resize(sunglasses, (w, h), interpolation = cv2.INTER_CUBIC)\n",
    "\n",
    "# get region of interest on the face to change\n",
    "roi_color = img[y:y+h,x:x+w]\n",
    "\n",
    "plt.imshow(roi_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
